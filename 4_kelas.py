# -*- coding: utf-8 -*-
"""4 Kelas

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TH8abwr5CH95DEPHnwAvp2ZBgIRJlfDQ
"""

path = "/content/drive/MyDrive/2023.Cabai/Dataset.Resize.Pad.4"

history = {}

"""# InceptionV3"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


SEED = 99

#Training, Validasi dan Testing => 70% : 10% : 20%
train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(path, label_mode='categorical', image_size=(299, 299), subset='both', validation_split=0.3, seed=SEED)

val_batches = tf.data.experimental.cardinality(val_ds)

test_ds = val_ds.take((2*val_batches) //3)
val_ds = val_ds.take((2*val_batches) //3)

#Training = train_y
train_y = []

for i,label in train_ds:
  for y in label:
    train_y.append(y.numpy())

np.unique(train_y, return_counts= True)

#Validasi = val_y

val_y = []


for i,label in val_ds:
  for y in label:
    val_y.append(y.numpy())

np.unique(val_y, return_counts= True)

#Testing = test_y

test_y = []

for i,label in test_ds:
  for y in label:
    test_y.append(y.numpy())

np.unique(test_y, return_counts= True)

model= tf.keras.applications.inception_v3.InceptionV3(include_top=True, weights=None, classes=4)

model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])

tf.keras.utils.plot_model(model)

model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',start_from_epoch=0)

history['inceptionv3'] = model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[callback])

model.save('model_inceptionv3')

plt.plot(history['inceptionv3'].history['accuracy'])
plt.plot(history['inceptionv3'].history['val_accuracy'])
plt.title('Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

# summarize history for loss
plt.plot(history['inceptionv3'].history['loss'])
plt.plot(history['inceptionv3'].history['val_loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

y_pred = model.predict(test_ds)

y_pred

predicted_categories = tf.argmax(y_pred, axis=1)

predicted_categories

print(classification_report(tf.argmax(test_y, axis=1), predicted_categories))

print(confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories))

# compute the confusion matrix
cm = confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories)

#Plot the confusion matrix.
sns.heatmap(cm,
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()


# Finding precision and recalltest_y, predicted_categories)
#print("Accuracy   :", history['inceptionv3'].history['accuracy'])

"""# VGG16"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


SEED = 99

#Training, Validasi dan Testing => 70% : 10% : 20%
train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(path, label_mode='categorical', image_size=(224, 224), subset='both', validation_split=0.3, seed=SEED)

val_batches = tf.data.experimental.cardinality(val_ds)

test_ds = val_ds.take((2*val_batches) //3)
val_ds = val_ds.take((2*val_batches) //3)

#train_y
train_y = []

for i,label in train_ds:
  for y in label:
    train_y.append(y.numpy())

np.unique(train_y, return_counts= True)

#val_y

val_y = []


for i,label in val_ds:
  for y in label:
    val_y.append(y.numpy())

np.unique(val_y, return_counts= True)

#test_y

test_y = []

for i,label in test_ds:
  for y in label:
    test_y.append(y.numpy())

np.unique(test_y, return_counts= True)

model= tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, classes=4, input_shape=(224, 224, 3))
model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])

tf.keras.utils.plot_model(model)

model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',start_from_epoch=0)

history['vgg16'] = model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[callback])

plt.plot(history['vgg16'].history['accuracy'])
plt.plot(history['vgg16'].history['val_accuracy'])
plt.title('Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

# summarize history for loss
plt.plot(history['vgg16'].history['loss'])
plt.plot(history['vgg16'].history['val_loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

model.save('model_vgg16')

y_pred = model.predict(test_ds)

y_pred

predicted_categories = tf.argmax(y_pred, axis=1)

predicted_categories

print(classification_report(tf.argmax(test_y, axis=1), predicted_categories))

print(confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories))

# compute the confusion matrix
cm = confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories)

#Plot the confusion matrix.
sns.heatmap(cm,
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()


# Finding precision and recalltest_y, predicted_categories)
#print("Accuracy   :", history['vgg16'].history['accuracy'])

"""# DenseNet-121"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

SEED = 99

#Training, Validasi dan Testing => 70% : 10% : 20%

train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(path, label_mode='categorical', image_size=(224, 224), subset='both', validation_split=0.3, seed=SEED)

val_batches = tf.data.experimental.cardinality(val_ds)

test_ds = val_ds.take((2*val_batches) //3)
val_ds = val_ds.take((2*val_batches) //3)

#train_y
train_y = []

for i,label in train_ds:
  for y in label:
    train_y.append(y.numpy())

np.unique(train_y, return_counts= True)

#val_y

val_y = []


for i,label in val_ds:
  for y in label:
    val_y.append(y.numpy())

np.unique(val_y, return_counts= True)

#test_y

test_y = []

for i,label in test_ds:
  for y in label:
    test_y.append(y.numpy())

np.unique(test_y, return_counts= True)

model= tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None, classes=4)

model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])

tf.keras.utils.plot_model(model)

model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',start_from_epoch=0)

history['densenet121'] = model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[callback])

model.save('model_densenet121')

plt.plot(history['densenet121'].history['accuracy'])
plt.plot(history['densenet121'].history['val_accuracy'])
plt.title('Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

# summarize history for loss
plt.plot(history['densenet121'].history['loss'])
plt.plot(history['densenet121'].history['val_loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

y_pred = model.predict(test_ds)

y_pred

predicted_categories = tf.argmax(y_pred, axis=1)

predicted_categories

print(classification_report(tf.argmax(test_y, axis=1), predicted_categories))

print(confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories))

# compute the confusion matrix
cm = confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories)

#Plot the confusion matrix.
sns.heatmap(cm,
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()


# Finding precision and recalltest_y, predicted_categories)
#print("Accuracy   :", history['densenet121'].history['accuracy'])

"""# AlexNet"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


SEED = 99

#Training, Validasi dan Testing => 70% : 10% : 20%

train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(path, label_mode='categorical', image_size=(224, 224), subset='both', validation_split=0.3, seed=SEED)

val_batches = tf.data.experimental.cardinality(val_ds)

test_ds = val_ds.take((2*val_batches) //3)
val_ds = val_ds.take((2*val_batches) //3)

#train_y
train_y = []

for i,label in train_ds:
  for y in label:
    train_y.append(y.numpy())

np.unique(train_y, return_counts= True)

#val_y

val_y = []


for i,label in val_ds:
  for y in label:
    val_y.append(y.numpy())

np.unique(val_y, return_counts= True)

#test_y

test_y = []

for i,label in test_ds:
  for y in label:
    test_y.append(y.numpy())

np.unique(test_y, return_counts= True)

model = models.Sequential()

# Add the Resizing layer as a preprocessing step for input data
model.add(layers.experimental.preprocessing.Resizing(224, 224, interpolation="bilinear", input_shape=(224, 224, 3)))

model.add(layers.Conv2D(96, 11, strides=4, padding='same'))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))

model.add(layers.Conv2D(256, 5, strides=4, padding='same'))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))

model.add(layers.Conv2D(384, 3, strides=4, padding='same'))
model.add(layers.Activation('relu'))

model.add(layers.Conv2D(384, 3, strides=4, padding='same'))
model.add(layers.Activation('relu'))

model.add(layers.Conv2D(256, 3, strides=4, padding='same'))
model.add(layers.Activation('relu'))

model.add(layers.Flatten())
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))

# The last dense layer should have 4 neurons since you mentioned num_classes = 4
model.add(layers.Dense(4, activation='softmax'))

# Print the model summary
model.summary()

model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',start_from_epoch=0)

history['alexnet'] = model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[callback])

plt.plot(history['alexnet'].history['accuracy'])
plt.plot(history['alexnet'].history['val_accuracy'])
plt.title('Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

# summarize history for loss
plt.plot(history['alexnet'].history['loss'])
plt.plot(history['alexnet'].history['val_loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
plt.draw()

model.save('model_alexnet')

y_pred = model.predict(test_ds)

y_pred

predicted_categories = tf.argmax(y_pred, axis=1)

predicted_categories

print(classification_report(tf.argmax(test_y, axis=1), predicted_categories))

print(confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories))

# compute the confusion matrix
cm = confusion_matrix(tf.argmax(test_y, axis=1), predicted_categories)

#Plot the confusion matrix.
sns.heatmap(cm,
            annot=True,
            fmt='g')
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()


# Finding precision and recalltest_y, predicted_categories)
#print("Accuracy   :", history['alexnet'].history['accuracy'])

"""# Pickle"""

import pickle

with open("4_kelas.pkl", "wb") as f:
  pickle.dump(history, f)

#protocol=pickle.HIGHEST_PROTOCOL

!cp 4_kelas.pkl /content/drive/MyDrive
!cp -r model_densenet121 /content/drive/MyDrive
!cp -r model_alexnet /content/drive/MyDrive
!cp -r model_inceptionv3 /content/drive/MyDrive
!cp -r model_vgg16 /content/drive/MyDrive